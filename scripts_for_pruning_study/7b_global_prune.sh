


# prune_ckpt_path='7b-mlp/llama-7b-mlp-0-31-0.8-8192Redpajama-global-first'
# echo "[START] - Start Pruning Model"
# deepspeed --master_port=43479 --include localhost:0,1,2,3,4,5,6,7 examples/prune_model.py --model_name_or_path /home/zhangyihan/.cache/huggingface/hub/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0 --pruning_ratio 0.8 --block_wise --device gpu --block_mlp_layer_start 0 --block_mlp_layer_end 31 --block_attention_layer_start 1 --block_attention_layer_end 1 --output_dir $prune_ckpt_path --pruner_type taylor --test_after_train --taylor param_first --save_model --grouping_strategy sum --num_examples 512 --prune_block_size 1024 --block_size 1024 --prune_batch_size 1 --torch_dtype float16 --deepspeed configs/ds_config_zero3_for_eval.json --dataset_path data/wikitext-103-raw-v1/test --pruning_dataset wikitext_cat --layer_importance "1.0227742260672568, 0.4187449028014783, 0.3889704304385315, 0.3216977340098863, 0.33805956583531876, 0.3425536000754512, 0.3322832713343088, 0.33381267411130283, 0.34699170049673717, 0.32327538993145327, 0.33805956583531876, 0.3395639288119467, 0.32011257416441, 0.28557439117210426, 0.26420658436860583, 0.27324603799606223, 0.25045400186741446, 0.23420131211473263, 0.20773775029228278, 0.19055113095391613, 0.17686565285763697, 0.15954292888895208, 0.12090408592752845, 0.14641814300826347, 0.11668522333218645, 0.12090408592752845, 0.09903541693516447, 0.09903541693516447, 0.12090408592752845, 0.11668522333218645, 0.11668522333218645, 0.15954292888895208" --grad_info_path /home/zhangyihan/LMFlow/grad_info/7b_8192red_first_grad_info.bin --exp_t 8.0 --global_pruning
# echo "[FINISH] - Finish Pruning Model"

# prune_ckpt_path='7b-mlp/llama-7b-mlp-0-31-0.25-8192Redpajama-global-second'
# echo "[START] - Start Pruning Model"
# deepspeed --master_port=43479 --include localhost:0,1,2,3,4,5,6,7 examples/prune_model.py --model_name_or_path /home/zhangyihan/.cache/huggingface/hub/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0 --pruning_ratio 0.25 --block_wise --device gpu --block_mlp_layer_start 0 --block_mlp_layer_end 31 --block_attention_layer_start 0 --block_attention_layer_end 0 --output_dir $prune_ckpt_path --pruner_type taylor --test_after_train --taylor param_second --save_model --grouping_strategy sum --num_examples 512 --prune_block_size 1024 --block_size 1024 --prune_batch_size 1 --torch_dtype float16 --deepspeed configs/ds_config_zero3_for_eval.json --dataset_path data/wikitext-103-raw-v1/test --pruning_dataset wikitext_cat --layer_importance "1.0227742260672568, 0.4187449028014783, 0.3889704304385315, 0.3216977340098863, 0.33805956583531876, 0.3425536000754512, 0.3322832713343088, 0.33381267411130283, 0.34699170049673717, 0.32327538993145327, 0.33805956583531876, 0.3395639288119467, 0.32011257416441, 0.28557439117210426, 0.26420658436860583, 0.27324603799606223, 0.25045400186741446, 0.23420131211473263, 0.20773775029228278, 0.19055113095391613, 0.17686565285763697, 0.15954292888895208, 0.12090408592752845, 0.14641814300826347, 0.11668522333218645, 0.12090408592752845, 0.09903541693516447, 0.09903541693516447, 0.12090408592752845, 0.11668522333218645, 0.11668522333218645, 0.15954292888895208" --grad_info_path /home/zhangyihan/LMFlow/grad_info/7b_8192red_second_grad_info.bin --exp_t 8.0 --global_pruning
# echo "[FINISH] - Finish Pruning Model"


# prune_ckpt_path='7b-mha+mlp/llama-7b-mha-1-32-mlp-0-31-0.25-8192Redpajama-global-first'
# echo "[START] - Start Pruning Model"
# deepspeed --master_port=43479 --include localhost:0,1,2,3,4,5,6,7 examples/prune_model.py --model_name_or_path prune_log/7b-mlp/llama-7b-mlp-0-31-0.25-8192Redpajama-global-first --pruning_ratio 0.25 --block_wise --device gpu --block_mlp_layer_start 0 --block_mlp_layer_end 0 --block_attention_layer_start 1 --block_attention_layer_end 32 --output_dir $prune_ckpt_path --pruner_type taylor --test_after_train --taylor param_first --global_pruning --save_model --grouping_strategy sum --num_examples 512 --prune_block_size 1024 --block_size 1024 --prune_batch_size 1 --torch_dtype float16 --deepspeed configs/ds_config_zero3_for_eval.json --dataset_path data/wikitext-103-raw-v1/test --pruning_dataset wikitext_cat --layer_importance "1.0227742260672568, 0.4187449028014783, 0.3889704304385315, 0.3216977340098863, 0.33805956583531876, 0.3425536000754512, 0.3322832713343088, 0.33381267411130283, 0.34699170049673717, 0.32327538993145327, 0.33805956583531876, 0.3395639288119467, 0.32011257416441, 0.28557439117210426, 0.26420658436860583, 0.27324603799606223, 0.25045400186741446, 0.23420131211473263, 0.20773775029228278, 0.19055113095391613, 0.17686565285763697, 0.15954292888895208, 0.12090408592752845, 0.14641814300826347, 0.11668522333218645, 0.12090408592752845, 0.09903541693516447, 0.09903541693516447, 0.12090408592752845, 0.11668522333218645, 0.11668522333218645, 0.15954292888895208" --grad_info_path /home/zhangyihan/LMFlow/grad_info/7b_8192red_first_grad_info.bin --exp_t 8.0 --arch_type pruned_decoder_only
# echo "[FINISH] - Finish Pruning Model"

# prune_ckpt_path='7b-mha+mlp/llama-7b-mha-1-32-mlp-0-31-0.25-8192Redpajama-global-second'
# echo "[START] - Start Pruning Model"
# deepspeed --master_port=43479 --include localhost:0,1,2,3,4,5,6,7 examples/prune_model.py --model_name_or_path prune_log/7b-mlp/llama-7b-mlp-0-31-0.25-8192Redpajama-global-second --pruning_ratio 0.25 --block_wise --device gpu --block_mlp_layer_start 0 --block_mlp_layer_end 0 --block_attention_layer_start 1 --block_attention_layer_end 32 --output_dir $prune_ckpt_path --pruner_type taylor --test_after_train --taylor param_second --global_pruning --save_model --grouping_strategy sum --num_examples 512 --prune_block_size 1024 --block_size 1024 --prune_batch_size 1 --torch_dtype float16 --deepspeed configs/ds_config_zero3_for_eval.json --dataset_path data/wikitext-103-raw-v1/test --pruning_dataset wikitext_cat --layer_importance "1.0227742260672568, 0.4187449028014783, 0.3889704304385315, 0.3216977340098863, 0.33805956583531876, 0.3425536000754512, 0.3322832713343088, 0.33381267411130283, 0.34699170049673717, 0.32327538993145327, 0.33805956583531876, 0.3395639288119467, 0.32011257416441, 0.28557439117210426, 0.26420658436860583, 0.27324603799606223, 0.25045400186741446, 0.23420131211473263, 0.20773775029228278, 0.19055113095391613, 0.17686565285763697, 0.15954292888895208, 0.12090408592752845, 0.14641814300826347, 0.11668522333218645, 0.12090408592752845, 0.09903541693516447, 0.09903541693516447, 0.12090408592752845, 0.11668522333218645, 0.11668522333218645, 0.15954292888895208" --grad_info_path /home/zhangyihan/LMFlow/grad_info/7b_8192red_second_grad_info.bin --exp_t 8.0 --arch_type pruned_decoder_only
# echo "[FINISH] - Finish Pruning Model"

prune_ckpt_path='7b-mha/llama-7b-mha-1-32-0.95-8192Redpajama-global-first'
echo "[START] - Start Pruning Model"
deepspeed --master_port=43472 --include localhost:0,1,2,3,4,5,6,7 examples/prune_model.py --model_name_or_path /home/zhangyihan/.cache/huggingface/hub/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0 --pruning_ratio 0.95 --block_wise --device gpu --block_mlp_layer_start 0 --block_mlp_layer_end 0 --block_attention_layer_start 1 --block_attention_layer_end 32 --output_dir $prune_ckpt_path --pruner_type taylor --test_after_train --taylor param_first --save_model --grouping_strategy sum --num_examples 512 --prune_block_size 1024 --block_size 1024 --prune_batch_size 1 --torch_dtype float16 --deepspeed configs/ds_config_zero3_for_eval.json --dataset_path data/wikitext-103-raw-v1/test --pruning_dataset wikitext_cat --layer_importance "1.0227742260672568, 0.4187449028014783, 0.3889704304385315, 0.3216977340098863, 0.33805956583531876, 0.3425536000754512, 0.3322832713343088, 0.33381267411130283, 0.34699170049673717, 0.32327538993145327, 0.33805956583531876, 0.3395639288119467, 0.32011257416441, 0.28557439117210426, 0.26420658436860583, 0.27324603799606223, 0.25045400186741446, 0.23420131211473263, 0.20773775029228278, 0.19055113095391613, 0.17686565285763697, 0.15954292888895208, 0.12090408592752845, 0.14641814300826347, 0.11668522333218645, 0.12090408592752845, 0.09903541693516447, 0.09903541693516447, 0.12090408592752845, 0.11668522333218645, 0.11668522333218645, 0.15954292888895208" --grad_info_path /home/zhangyihan/LMFlow/grad_info/7b_8192red_first_grad_info.bin --exp_t 8.0 --global_pruning
echo "[FINISH] - Finish Pruning Model"

# prune_ckpt_path='7b-mha/llama-7b-mha-1-32-0.25-8192Redpajama-local-second'
# echo "[START] - Start Pruning Model"
# deepspeed --master_port=43479 --include localhost:0,1,2,3,4,5,6,7 examples/prune_model.py --model_name_or_path /home/zhangyihan/.cache/huggingface/hub/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0 --pruning_ratio 0.25 --block_wise --device gpu --block_mlp_layer_start 0 --block_mlp_layer_end 0 --block_attention_layer_start 1 --block_attention_layer_end 32 --output_dir $prune_ckpt_path --pruner_type taylor --test_after_train --taylor param_second --save_model --grouping_strategy sum --num_examples 512 --prune_block_size 1024 --block_size 1024 --prune_batch_size 1 --torch_dtype float16 --deepspeed configs/ds_config_zero3_for_eval.json --dataset_path data/wikitext-103-raw-v1/test --pruning_dataset wikitext_cat --layer_importance "1.0227742260672568, 0.4187449028014783, 0.3889704304385315, 0.3216977340098863, 0.33805956583531876, 0.3425536000754512, 0.3322832713343088, 0.33381267411130283, 0.34699170049673717, 0.32327538993145327, 0.33805956583531876, 0.3395639288119467, 0.32011257416441, 0.28557439117210426, 0.26420658436860583, 0.27324603799606223, 0.25045400186741446, 0.23420131211473263, 0.20773775029228278, 0.19055113095391613, 0.17686565285763697, 0.15954292888895208, 0.12090408592752845, 0.14641814300826347, 0.11668522333218645, 0.12090408592752845, 0.09903541693516447, 0.09903541693516447, 0.12090408592752845, 0.11668522333218645, 0.11668522333218645, 0.15954292888895208" --grad_info_path /home/zhangyihan/LMFlow/grad_info/7b_8192red_second_grad_info.bin --exp_t 8.0
# echo "[FINISH] - Finish Pruning Model"